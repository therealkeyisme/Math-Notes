<div data-align="CENTER">
<strong><strong>A Summary of Linear Algebra</strong></strong><br />
<a href="../..">John Mitchell</a>
</div>
<p>This document is a list of some material in linear algebra that you should be familiar with. Throughout, we will take <em>A</em> to be the 3 x 4 matrix<br />
</p>
<div data-align="CENTER">
<img src="img1.gif" width="214" height="76" alt="\begin{displaymath} A = \left[ \begin{array}{rrrr} 1 &amp; 2 &amp; 3 &amp; 4 \\ -2 &amp; 3 &amp; -1 &amp; 5 \\ 3 &amp; -1 &amp; 4 &amp; -1 \end{array} \right] \end{displaymath}" />
</div>
<p><br />
</p>
<p>I assume you are familiar with matrix and vector addition and multiplication.</p>
<ul>
<li>All vectors will be <strong>column</strong> vectors.</li>
<li>Given a vector <em>v</em>, if we say that <img src="img2.gif" width="48" height="34" alt="$v\neq 0$" />, we mean that <em>v</em> has at least one nonzero component.</li>
<li>The <strong>transpose</strong> of a vector or matrix is denoted by a superscript <em>T</em>. For example,<br />

<div data-align="CENTER">
<img src="img3.gif" width="170" height="99" alt="\begin{displaymath} A^T = \left[ \begin{array}{rrr} 1 &amp; -2 &amp; 3 \\ 2 &amp; 3 &amp; -1 \\ 3 &amp; -1 &amp; 4 \\ 4 &amp; 5 &amp; -1 \end{array} \right] \end{displaymath}" />
</div>
<br />
</li>
<li>The <strong>inner product</strong> or <strong>dot product</strong> of two vectors <em>u</em> and <em>v</em> in <img src="img4.gif" width="31" height="17" alt="$I \! \! R^n$" /> can be written <em>u</em><sup><em>T</em></sup><em>v</em>; this denotes <img src="img5.gif" width="77" height="34" alt="$\sum_{i=1}^n u_iv_i$" />. If <em>u</em><sup><em>T</em></sup><em>v</em>=0 then <em>u</em> and <em>v</em> are <strong>orthogonal</strong>.</li>
<li>The <strong>null space</strong> of <em>A</em> is the set of all solutions <em>x</em> to the matrix-vector equation <em>Ax</em>=0.</li>
<li>To solve a system of equations <em>Ax</em>=<em>b</em>, use Gaussian elimination. For example, if <img src="img6.gif" width="126" height="39" alt="$b=[4,\: -3,\: 7]^T$" />, then we solve <em>Ax</em>=<em>b</em> as follows: (We set up the augmented matrix and row reduce (or pivot) to upper triangular form.)<br />

<div data-align="CENTER">
<img src="img7.gif" width="556" height="77" alt="\begin{displaymath} \begin{tabular}{\vert rrrr\vert r\vert} \hline 1 &amp; 2 &amp; 3 ... ... &amp; 7 &amp; 5 &amp; 13 &amp; 5 \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\ \hline \end{tabular}\end{displaymath}" />
</div>
<br />

Thus, the solutions are all vectors <em>x</em> of the form<br />

<div data-align="CENTER">
<img src="img8.gif" width="266" height="99" alt="\begin{displaymath} x=\left[\begin{array}{r}1\\ 0\\ 1\\ 0\end{array}\right]+ s... ...ht]+ t\left[\begin{array}{r}2\\ 13\\ 0\\ -7\end{array}\right] \end{displaymath}" />
</div>
<br />

for any numbers <em>s</em> and <em>t</em>.</li>
<li>The <strong>span</strong> of a set of vectors is the set of all linear combinations of the vectors. For example, if <img src="img9.gif" width="167" height="39" alt="$v^1=[11,\: 5,\: -7,\: 0]^T$" /> and <img src="img10.gif" width="167" height="39" alt="$v^2=[2,\: 13,\: 0,\: -7]^T$" /> then the span of <em>v</em><sup>1</sup> and <em>v</em><sup>2</sup> is the set of all vectors of the form <em>sv</em><sup>1</sup>+<em>tv</em><sup>2</sup> for some scalars <em>s</em> and <em>t</em>.</li>
<li>The span of a set of vectors in <img src="img4.gif" width="31" height="17" alt="$I \! \! R^n$" /> gives a <strong>subspace</strong> of <img src="img4.gif" width="31" height="17" alt="$I \! \! R^n$" />. Any nontrivial subspace can be written as the span of any one of uncountably many sets of vectors.</li>
<li>A set of vectors <img src="img11.gif" width="105" height="38" alt="$\{v^1,\ldots,v^m\}$" /> is <strong>linearly independent</strong> if the only solution to the vector equation <img src="img12.gif" width="188" height="38" alt="$\lambda_1v^1+\ldots+\lambda_mv^m=0$" /> is <img src="img13.gif" width="55" height="34" alt="$\lambda_i=0$" /> for all <em>i</em>. If a set of vectors is not linearly independent, then it is <strong>linearly dependent</strong>. For example, the rows of <em>A</em> are <em>not</em> linearly independent, since<br />

<div data-align="CENTER">
<img src="img14.gif" width="311" height="99" alt="\begin{displaymath} - \left[ \begin{array}{r}1\\ 2\\ 3\\ 4\end{array} \right] ... ...t] = \left[ \begin{array}{r}0\\ 0\\ 0\\ 0\end{array} \right]. \end{displaymath}" />
</div>
<br />

To determine whether a set of vectors is linearly independent, write the vectors as columns of a matrix <em>C</em>, say, and solve <em>Cx</em>=0. If there are any nontrivial solutions then the vectors are linearly dependent; otherwise, they are linearly independent.</li>
<li>If a linearly independent set of vectors spans a subspace then the vectors form a <strong>basis</strong> for that subspace. For example, <em>v</em><sup>1</sup> and <em>v</em><sup>2</sup> form a basis for the span of the rows of <em>A</em>. Given a subspace <em>S</em>, every basis of <em>S</em> contains the same number of vectors; this number is the <strong>dimension</strong> of the subspace. To find a basis for the span of a set of vectors, write the vectors as rows of a matrix and then row reduce the matrix.</li>
<li>The span of the rows of a matrix is called the <strong>row space</strong> of the matrix. The dimension of the row space is the <strong>rank</strong> of the matrix.</li>
<li>The span of the columns of a matrix is called the <strong>range</strong> or the <strong>column space</strong> of the matrix. The row space and the column space always have the same dimension.</li>
<li>If <em>M</em> is an <em>m</em> x <em>n</em> matrix then the null space and the row space of M are subspaces of <img src="img4.gif" width="31" height="17" alt="$I \! \! R^n$" /> and the range of M is a subspace of <img src="img15.gif" width="35" height="17" alt="$I \! \! R^m$" />.</li>
<li>If <em>u</em> is in the row space of a matrix <em>M</em> and <em>v</em> is in the null space of <em>M</em> then the vectors are orthogonal. The dimension of the null space of a matrix is the <strong>nullity</strong> of the matrix. If <em>M</em> has <em>n</em> columns then rank(<em>M</em>)+nullity(<em>M</em>)=<em>n</em>. Any basis for the row space together with any basis for the null space gives a basis for <img src="img4.gif" width="31" height="17" alt="$I \! \! R^n$" />.</li>
<li>If <em>M</em> is a square matrix, <img src="img16.gif" width="15" height="18" alt="$\lambda$" /> is a scalar, and <em>x</em> is a vector satisfying <img src="img17.gif" width="81" height="18" alt="$Mx=\lambda x$" /> then <em>x</em> is an <strong>eigenvector</strong> of <em>M</em> with corresponding <strong>eigenvalue</strong> <img src="img16.gif" width="15" height="18" alt="$\lambda$" />. For example, the vector <img src="img18.gif" width="92" height="39" alt="$x=[1,\: 2]^T$" /> is an eigenvector of the matrix<br />

<div data-align="CENTER">
<img src="img19.gif" width="109" height="55" alt="\begin{displaymath} M = \left[ \begin{array}{rr}3&amp;2\\ 2&amp;6\end{array}\right] \end{displaymath}" />
</div>
<br />

with eigenvalue <img src="img20.gif" width="49" height="18" alt="$\lambda=7$" />.</li>
<li>The eigenvalues of a symmetric matrix are always real. A nonsymmetric matrix may have complex eigenvalues.</li>
<li>Given a symmetric matrix <em>M</em>, the following are equivalent:
<dl>
<dt>1.</dt>
<dd>All the eigenvalues of <em>M</em> are positive.
</dd>
<dt>2.</dt>
<dd><em>x</em><sup><em>T</em></sup><em>Mx</em>&gt;0 for any <img src="img21.gif" width="49" height="34" alt="$x \neq 0$" />.
</dd>
<dt>3.</dt>
<dd>M is <strong>positive definite</strong>.
</dd>
</dl></li>
<li>Given a symmetric matrix <em>M</em>, the following are equivalent:
<dl>
<dt>1.</dt>
<dd>All the eigenvalues of <em>M</em> are nonnegative.
</dd>
<dt>2.</dt>
<dd><img src="img22.gif" width="90" height="39" alt="$x^TMx \geq 0$" /> for any <em>x</em>.
</dd>
<dt>3.</dt>
<dd>M is <strong>positive semidefinite</strong>.
</dd>
</dl></li>
</ul>
<p><br />
</p>
<hr />
<p><span id="CHILD_LINKS"> </span></p>
<ul>
<li><a href="node1.html" id="tex2html3">About this document ...</a></li>
</ul>
<hr />
<p><a href="../..">John E. Mitchell</a><br />
<em>2004-08-31</em></p>
