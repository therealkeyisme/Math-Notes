\section{Orthogonality}

\subsection{}

  Recall that 

  \begin{equation}
    \vec{v}\cdot\vec{w}=v_1\cdot w_1+\ldots+v_n\cdot w_n = | | \vec{v} | | | | \vec{w} | | \cos(\theta)
  ,\end{equation}

  where $\theta$ is the angle between $\vec{v}$ and $\vec{w}$. Because $\cos\left(\frac{\pi}{2}\right)=0$, we know that $\vec{v}\cdot\vec{w}=0$ if and only if $\vec{v}$ is orthogonal to $\vec{w}$. In general, given $\vec{v},\vec{w}\in\R^n$, $\vec{v}$ is orthogonal to $\vec{w}$ if and only if the angle between them is $\frac{\pi}{2}$.

  \begin{definition}
    Let $U$ be any inner product space. A basis $\vec{u},\ldots,\vec{u_h}\in V$ is orthogonal if $\vec{u_j}\cdot \vec{u_i} = 0$ whenever $i\neq j$.\newline
    In addition, if $| |u_i| |=1$ for all $i$'s, the basis is orthonormal.
  \end{definition}
  
  Here are a few examples of orthonormal and orthogonal basis's:

  \begin{enumerate}
    \item $\SmallMatrix{1\\0}\SmallMatrix{0\\1}$ is an orthonormal basis.
    \item $\SmallMatrix{2\\0}\SmallMatrix{0\\1}$ is an orthogonal basis.
    \item $\SmallMatrix{1\\0\\0}\SmallMatrix{0\\1\\0}\SmallMatrix{0\\0\\1}$ is an orthonormal basis.
  \end{enumerate}

  If $\vec{v_1},\ldots,\vec{v_n}$ is an orthogonal basis, then 

  \begin{equation}
    \frac{\vec{v_1}}{| |\vec{v_1}| |},\frac{\vec{v_2}}{| | \vec{v_2} | |},\ldots,\frac{\vec{v_n}}{| | v_n | |}
  \end{equation}
  is an orthonormal basis.

  \begin{problem}
    Consider $\vec{u_1}=\SmallMatrix{1\\2}$ and $\vec{u_2}=\SmallMatrix{2\\-1}$. We know that these two matrices are not linearly dependent because $\vec{u_1}$ is not a multiple of $\vec{u_2}$. We can see that this is an orthogonal basis.
    \begin{equation}
      | | \vec{u_1} | | = \sqrt{1^2+2^2} =\sqrt{5} 
    \end{equation}

    so we know

    \begin{equation}
      v_1=\begin{pmatrix} \frac{1}{\sqrt{5} }\\\frac{1}{\sqrt{5} } \end{pmatrix} ,v_2=\begin{pmatrix} \frac{1}{\sqrt{5} }\\\frac{1}{\sqrt{5} } \end{pmatrix} 
    \end{equation}
  \end{problem}

  \begin{prop}
    Assume $\vec{v_1},\ldots,\vec{v_n}\in V$ with $\vec{v_i}\neq \vec{0}$ for all $i$. Assume that $<v_i,v_j\ge 0$ whenever $i\neq j$, then $\{v_1,\ldots,v_n\}$ is linearly independent.
    \begin{proof}
      Assume that $c_1\vec{v_1}+\ldots+c_n\vec{n}=0$. Let $i\in\{1,\ldots,n\}$
      \begin{align*}
        <c_1\vec{v_1}+\ldots+c_n \vec{v_n}, v_i> = <0,\vec{v_i}>\\
        c_1<v_1,v_i> + c_2<v_2,v_i>+\ldots+ c_i<v_i,v_i>+\ldots+c_n<v_i,v_n>\\
        c_i<v_i,v_i> = 0\\
        c_i = 0
      .\end{align*}
      Now we know that $v_1,\ldots,v_n$ are linearly independent.
    \end{proof}
  \end{prop}

  \begin{corollary}
    If $dim(v)=n$ and $\vec{v_1},\ldots,\vec{v_n}$ are $n$ vectors such that $<v_i,v_j> = 0$ wherever $i\neq j$, then $\{\vec{v_1},\ldots,\vec{v_n}\}$ is a basis.
  \end{corollary}

  \begin{theorem}
    Let $\vec{u_1},\ldots,\vec{u_n}$ be an orthonormal basis for $v$. Let $\vec{v}\in V$. Then we know $\vec{v}=c_1\vec{u_1}+\ldots+c_n \vec{u_n}$. In fact, $c_i=<\vec{v},\vec{u_i}>$ and 
    \begin{equation}
      | |\vec{v} | | = \sqrt{<\vec{v},\vec{u_1},>^2+<\vec{v},\vec{u_2}^2 + <\vec{v},\vec{u_n}>^2}.
    \end{equation}
  \end{theorem}

  \begin{problem}
    $\mathbb{P}^2$ polynomials of degree $\le 2$ on [0,1]. Use the $L^2$ norm.

    \begin{equation}
      <p,q> = \int_0^{1}p(x)q(x)dx
    \end{equation}

    Let $p_1=1,p_2=x-\frac{1}{2},p_3=x^2-x+\frac{1}{6}$.

    \begin{align}
      <p_1,p_2> &= \int_0^{1}x-\frac{1}{2}dx=0\\
      <p_1,p_3>&=\int_0^{1}x^2-x+\frac{1}{6}dx=0.
    \end{align}

    We have an orthogonal basis because of this. In order to check to see if it is orthonormal we must also do $<p_1,p_1>,<p_2,p_2>,<p_3,p_3>$
  \end{problem}
  INSERT NOTES FROM 03.08 HERE

  So if we have the basis $\{\SmallMatrix{1\\2},\SmallMatrix{2\\1}\}, v_1=\SmallMatrix{1\\2}$. $v_2$ is equal to 

  \begin{align*}
    \begin{pmatrix} 2\\1 \end{pmatrix} -\frac{\Big<\begin{pmatrix} 2\\1 \end{pmatrix} , \begin{pmatrix} 1\\2 \end{pmatrix} \Big>}{| | \begin{pmatrix} 1\\2 \end{pmatrix} }\cdot \begin{pmatrix} 1\\2 \end{pmatrix} \\
    \begin{pmatrix} 2\\1 \end{pmatrix} -\frac{4}{\left(\sqrt{1^2+2^2} \right)^2 }\\
    \begin{pmatrix} 2\\1 \end{pmatrix} -\frac{4}{5}\begin{pmatrix} 1\\2 \end{pmatrix}\\ 
    \begin{pmatrix} 2\\1 \end{pmatrix} -\begin{pmatrix} \frac{4}{5}\\ \frac{8}{5} \end{pmatrix} = \begin{pmatrix} \frac{10}{5}\\\frac{5}{5} \end{pmatrix} -\begin{pmatrix} \frac{4}{5}\\\frac{8}{5} \end{pmatrix} \\
    =\begin{pmatrix} \frac{6}{3} \\ -\frac{3}{5}\end{pmatrix} 
    \SmallMatrix{2\\1}-\frac{<\SmallMatrix{2\\1}, \SmallMatrix{1\\2}>}{| | \SmallMatrix{1\\2}| | ^2}\cdot \SmallMatrix{1\\2}
    \SmallMatrix{2\\1}-\frac{4}{(\sqrt{1^2+2^2} )^2}\\
    \SmallMatrix{2\\1} - \frac{4}{5}\SmallMatrix{1\\2}\\
    \SmallMatrix{2\\1}-\SmallMatrix{\frac{4}{5}\\\frac{8}{5}}=\SmallMatrix{\frac{10}{5}\\\frac{5}{5}}-\SmallMatrix{\frac{4}{5}\\\frac{8}{5}}\\
    \SmallMatrix{\frac{6}{3}\\-\frac{3}{5}}\\
  .\end{align*}

  We can conclude that our basis is $\{\SmallMatrix{1\\2},\SmallMatrix{\frac{6}{5}\\-\frac{3}{5}}\}$. Let's now use our basis and rewrite it as
  \[
  \begin{pmatrix} 2\\3 \end{pmatrix} =c_1\begin{pmatrix} 1\\2 \end{pmatrix} +c_2\begin{pmatrix} \frac{6}{5}\\-\frac{3}{5} \end{pmatrix} 
  .\] 
  We can simplify this and solve for $c_1,c_2$

  \begin{align*}
    c_1&=<\begin{pmatrix} 2\\3 \end{pmatrix} ,\begin{pmatrix} 1\\2 \end{pmatrix}> = 2+6=8\\
    c_2&=<\begin{pmatrix} 2\\3 \end{pmatrix} ,\begin{pmatrix} \frac{6}{5}\\-\frac{3}{5} \end{pmatrix}> = \frac{12}{5}-\frac{9}{5}=\frac{3}{5}\\
  .\end{align*}
  Using Theorem 4.9 from the book, we do the following with an orthogonal basis to get its norm:

  \begin{align*}
    a_1=\frac{8}{| | v_1| |^2}=\frac{8}{(\sqrt{1^2+2^2} )^2}=\frac{8}{5}\\
    a_2=\frac{\frac{3}{5}}{| | v_2 | |^2}=\frac{\frac{3}{5}}{\left(\frac{6}{5}\right)^2+\left(-\frac{3}{5}\right)^2}\\
    =\frac{\frac{3}{5}}{\frac{36}{25}+\frac{9}{25}}=\frac{15}{25}\ldots\\
    \begin{pmatrix} 2\\3 \end{pmatrix} =a_1\begin{pmatrix} 1\\2 \end{pmatrix} +a_2\begin{pmatrix} \frac{6}{5}\\-\frac{3}{5} \end{pmatrix} \\
    \frac{8}{5}\begin{pmatrix} 1\\2 \end{pmatrix} + \frac{1}{3}\begin{pmatrix} \frac{6}{5}\\-\frac{3}{5} \end{pmatrix} \\
    =\begin{pmatrix} \frac{8}{5}\\\frac{16}{5} \end{pmatrix} +\begin{pmatrix} \frac{2}{5}\\-\frac{1}{5} \end{pmatrix} =\begin{pmatrix} 2\\3 \end{pmatrix} 
  .\end{align*}

  \begin{problem}
    Example of an orthogonal basis. Let $\mathbb{T}^{n}$ be the vector space of trigonometric polynomials.
    \[
      \mathbb{T}^{n}=\sum_{0\le j+k\le n}a_{jk}\sin^{j}(x)\cos^k(x)\\
    .\] 
    Using the $L^2$ norm:
    \[
      <f,g> = \int_{-\pi}^{\pi}f\cdot g
    .\] 
    An orthogonal basis is $\{1,\cos(x),\sin(x),\cos(2x),\sin(2x),\cos(3x),\sin(3x),\ldots\}$. If we were going to do the $L^2$ norm for any of these equations we would need to do the following
    \[
      \int_{-\pi}^{\pi}\sin(2x)\cos(4x)dx
    .\] 
    This equation is the Fourier series.
  \end{problem}

\subsection{}
\subsection{Orthogonal Matrices}

  \begin{definition}
    AA square matrix $Q$ is orthogonal if $Q^{t}Q=Q\cdot Q^{T}=I$
  \end{definition}

  If $Q$ is orthogonal, then
  \begin{itemize}
    \item $Q^{-1}=Q^{T}$
    \item $det(Q)=\pm 1$
    \item $Q\cdot Q^{t}=I$
    \item $det(Q)det(Q^{T})=det(I)$
    \item $(det(Q))^2=1$
    \item $det(Q)=\pm 1$
  \end{itemize}

  Let $A=\begin{pmatrix} a_{11}&a_{12}\\a_{21}&a_{22} \end{pmatrix} $. Suppose that $A$ is orthogonal, then 
  \begin{align*}
    \begin{pmatrix} A_{11}&A_{12}\\A_{21}&A_{22} \end{pmatrix}\cdot\begin{pmatrix} A_{11}&A_{21}\\A_{12}&A_{22} \end{pmatrix} &=\begin{pmatrix} 1&0\\0&1 \end{pmatrix} \\
    a_{11}^2+a_{12}^2&=1\\
    a_{11}a_{21}+a_{12}a_{22}&=0\\
    a_{21}a_{11}+a_{22}a_{12}&=0\\
    a_{21}^2+a_{22}^2&=1
  .\end{align*}
  
  If we plot $(a_{11},a_{12})$ on a graph, we can see that $\cos\left( \theta \right) =a_{12}$ and $\sin(\theta)=a_{11} $

  \begin{prop}
    $Q$ is orthogonal if and only if its columns form an orthonormal basis.
    \begin{proof}
      Let $Q=[U_1\vdots U_2 \vdots\ldots\vdots U_n]$.
      \[
      Q^{T}=\begin{bmatrix} U_1^{T}\\U_2^{T}\\ \vdots \\ U_n^{T} \end{bmatrix} 
      .\] 
      In $Q^{T}Q$, the $i,j^{th}$ entry is $U_1^{T}\cdot Uj$.
      \[
      U_i^{T}\cdot U_j =
      \begin{cases}
        0&i\neq j\\
        1&i=j
      \end{cases}
      .\] 
      So the $U_i$'so form an orthonormal basis.
    \end{proof}
  \end{prop}
  \begin{problem}
    Let $A=\begin{bmatrix} 3&5\\7&\frac{1}{2} \end{bmatrix} $, and let $A$ be orthonormal. We know that $A\cdot A^{T}=A^{T}\cdot A=I$. Let's see if $A$ is an orthonormal basis.
    \[
      \begin{bmatrix} 3&7\\5&\frac{1}{2} \end{bmatrix} \begin{bmatrix} 3&5\\7&\frac{1}{2} \end{bmatrix} \neq \begin{bmatrix} 1&0\\0&1 \end{bmatrix} 
    .\] 
      This is not equal because $3\times 7+3\times 5\neq0$. Now let's try letting $A=\begin{bmatrix} 3&-7\\7&3 \end{bmatrix} $.
      \[
        \begin{bmatrix} 3&7\\-7&3 \end{bmatrix} \begin{bmatrix} 3&-7\\7&3 \end{bmatrix} =\begin{bmatrix} 1&0\\0&1 \end{bmatrix} 
      .\] 
      This vector works with the zero values, but not the ones values, so we need to normalize this vector.
      \begin{align*}
        \left| \left| \begin{bmatrix} 3\\7 \end{bmatrix}  \right|  \right|=\sqrt{4^2+7^2} =\sqrt{58} && \left| \left| \begin{bmatrix} -7\\3 \end{bmatrix}  \right|  \right|=\sqrt{(-7)^2+3^2} =\sqrt{58}
      \end{align*}
      \[
        A=\begin{bmatrix} \frac{3}{58}&-\frac{7}{58}\\\frac{7}{58}&\frac{3}{58} \end{bmatrix} 
      .\] 
  \end{problem}
  
        Let's let $B=\begin{pmatrix} a&b\\c&d \end{pmatrix} $, and suppose $Q$ is orthogonal.
        \[
          Q^{T}\cdot Q=\begin{pmatrix} a&c\\b&d \end{pmatrix} \begin{pmatrix} a&b\\c&d \end{pmatrix} =\begin{pmatrix} 1&0\\0&1 \end{pmatrix} 
        .\] 
        \begin{align*}
          a^2+c^2=1\\
          ab+cd=0\\
          ab+cd=0\\
          b^2+d^2=1
        .\end{align*}
        Given that the vectors $\begin{bmatrix} a\\c \end{bmatrix} ,\begin{bmatrix} b\\d \end{bmatrix} $ lie on the unit circle, we can determine that
        \begin{align*}
          a=\cos\theta&&c=\sin \theta\\
          b=\cos\phi&&d=\sin\phi
        ,\end{align*}
        and we can determine that 
        \[
          0=ab+cd=\cos\theta\cos\phi + \sin \theta\sin\phi=\cos(\theta-\phi)
        .\] 
        If we use $\cos(\theta-\phi),$ we can determine that $\phi = \theta\pm\pi$, so $b=-\sin \theta, d=\cos\theta$ or $b=\sin \theta, d=-\cos\theta$. We either have $Q$ in one of two forms.
        \[
          \begin{pmatrix} \cos\theta&&-\sin \theta\\ \sin \theta&&\cos\theta \end{pmatrix}\text{ or }\begin{pmatrix} \cos\theta && \sin \theta \\ \sin \theta &&-\cos\theta \end{pmatrix}  
        .\] 
        The determinant of the left matrix is 1, and the determinant of the right matrix is -1. They both give us a counter clockwise rotation by $\theta$, gives a reflection across the line with angle $\frac{\theta}{2}$

orthogonal matrices are square and $Q^{t}\cdot Q=Q Q^{t}=I$. Every $2\times 2$ orthogonal matrix has the form 
\[
  \begin{pmatrix} \cos\theta&&-\sin \theta\\ \sin \theta && \cos \theta \end{pmatrix} \text{ or } \begin{pmatrix} \cos\theta&& \sin \theta\\ \sin \theta && -\cos\theta \end{pmatrix} .
.\] 

In general, if $Q$ is orthogonal, then $det(Q)=\pm 1$

\begin{theorem}
  The product of two orthogonal matrices is orthogonal.
  Recall, if $Q$ is orthogonal, then $Q^{-1}=Q^{T}$. The orthogonal $n\times n$ matrices satisfy 
  \begin{itemize}
    \item Closed under the dot product.
    \item Multiplication is associative.
    \item They all have inverses.
    \item The identity matrix is orthogonal.
  \end{itemize}

\end{theorem}
\subsection{Vector Spaces}
Up until now we've been trying to get a basis to be able to establish a location of vectors. Once we have determined an inner product, we can find an angle. Let $V$ be a subspace. Let $W \le V$ be a finite dimensional subspace.
\begin{definition}
  $\vec{z}\in V$ is orthogonal to $w$ if it is orthogonal to every vector in $w$.
\end{definition}
\begin{note}
  If $\vec{w_1},\ldots,\vec{w_n}$ is a basis for $w$, then $\vec{z}$ is orthogonal to $w$ if and only if $\vec{z}$ is orthogonal to $w_1,\ldots,w_n$.
\end{note}
\begin{definition}
  The orthogonal projection of $\vec{V}$ onto $w$ is the vector $\vec{w}$ such that $\vec{z}=\vec{v}-\vec{w}$, where $\vec{z}$ is orthogonal to $w$.
\end{definition}
\begin{theorem}
  Let $\vec{u_1},\ldots,\vec{u_n}$ be an orthogonal basis for $w$. Let $\vec{v}\in V$. The orthogonal projection of $\vec{v}$ onto $w$ is 
  \[
    \vec{w}=c_1\vec{u_1}+\ldots+c_n \vec{u_n}\text{ where $c_i<v,v_i>$}
  .\] 
\end{theorem}
\begin{note}
  If $\vec{v_1},\ldots,\vec{v_n}$ is an orthogonal basis for $w$ then 
  \begin{align*}
    w=a_1\vec{v_1}+\ldots+a_n \vec{v_n}\\
    a_i=\frac{\left<\vec{v},\vec{v_i} \right>}{\|v_i\|^2}
  .\end{align*}
\end{note}
\begin{definition}
  Let $w,z\le V$ be subspaces. $w$ is orthogonal to $z$ if every vector in $w$ is orthogonal to every vector in $z$. For example,
    \[
    <\vec{w},\vec{z}> = 0
    \] 
   for every $\vec{w}\in w, \vec{z}\in z$ .
\end{definition}
\begin{note}
  We only need to show this is true on the bases of $w$ and $z$.
\end{note}
\begin{definition}
  Let $w\in v$ be a subspace. The orthogonal compliment of $w$, written $w^{T}$ is the set of vectors in $v$ orthogonal to $w$.
  \[
  w^{T}=\left\{ \vec{v}\in v | \left<\vec{v},\vec{w} \right> =0, \forall w\in w \right\} 
  .\] 
\end{definition}
\begin{theorem}
  Let $w<v$ be a finite dimensional subspace. Every $\vec{v}\in v$ can be written uniquely as 
  \[
  \vec{v}=\vec{w}+\vec{z}
  \] 
  where $\vec{w}\in w$ and $\vec{z}\in w^{T}$.
\end{theorem}
