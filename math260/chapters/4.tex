\newpage
\section{Systems of Ordinary Differential Equations}
\subsection{}
Systems of ordinary differential equations have 1 dependent variable and 1 independent variable. Let's look at an example.

\begin{eg}
  Here is a system of 3 first order ordinary differential equations of dimension 3 
  \begin{align*}
    x'&=4x-7y+z^2\\
    y'&=2t-e^{t}y-z\\
    z'&=z^2+y^2-yx
  .\end{align*}
\end{eg}

We can also turn higher order ODE's into systems of first-order ODEs.
\begin{eg}
  Consider the equation $x''+7x''+7tx'+6x=t^2$. If we let $y=x',z=x''=y',x'''=z'$, then using the ODE,
  \[
  z'=t^2-6x+7ty-7z
  ,\]
  we can make a system of nonlinear, nonautonomous, nonhomogeneous ordinary differential equations.\footnote{After this system we went to chapter 2.6}
  \begin{align*}
    x'&=y\\
    y'&=z\\
    z'&=t^2-6x+ty-7z
  .\end{align*}
\subsection{Matrix Algebra}
A matrix is a rectangular array of numbers. 
\begin{eg}
  Consider $A=\begin{pmatrix} 2&3&1\\4&5&0 \end{pmatrix} $. $A$ has 2 rows and 3 columns, so its size is $2\times 3$. The $a_{ij}$ is an entry in $A$'s $i^{th}$ row and $j^{th}$ column.
\end{eg}
\begin{eg}
  Consider $B=\begin{pmatrix} 5&0&0\\0&-1&0\\0&0&3 \end{pmatrix} $, which is a $3\times 3$ matrix. This is also a square matrix, due to the size. $B$ is also a diagonal matrix, because it's only nonzero entries are on the main diagonals.
\end{eg}
An upper triangular matrix looks like 
\[
  \begin{pmatrix} 5&2&0\\0&-1&3\\0&0&3 \end{pmatrix} 
.\] 
A lower triangular matrix looks like
\[
  \begin{pmatrix} 5&0&0\\ \pi&-1&0\\-1&-\frac{1}{2}&3 \end{pmatrix} 
.\] 
\end{eg}
We can add and subtract matrices element-wise, but only if the matrices have the same size. We can also multiply a matrix by a scalar
\[
  3\begin{pmatrix} 4&5\\6&7 \end{pmatrix} =\begin{pmatrix} 12&15\\18&21 \end{pmatrix} 
.\] 
A row vector is $\begin{pmatrix} 2&3&1 \end{pmatrix} $, in this case the size would be $1\times 3$. A column vector looks like $\begin{pmatrix} 2\\3\\1 \end{pmatrix} $ with the size $3\times 1$. The zero vector is of size $n\times n$, and has $0$ in every index in the matrix. The transpose of a matrix reverses all of the columns and rows. Consider
\[
  \begin{pmatrix} 2&3&1\\4&5&6 \end{pmatrix} ^{T}=\begin{pmatrix} 2&4\\3&5\\1&6 \end{pmatrix} 
.\] 
Here are the properties of a transpose:
\begin{itemize}
  \item $\left( A^{T} \right)^{T}=A $
  \item $\left( A+B \right) ^{T}=A^{T}+B^{T}$
  \item $(kA)^{T}=kA^{T}$
  \item $\left( AB \right) ^{T}=B^{T}A^{T}$
\end{itemize}
Now we are going to start talking about matrix multiplication. Matrix multiplication is like the dot product. The $(i,j)^{th}$ entry of $AB$ is the dot product of the $i^{th}$ row of $A$ w/ $j^{th}$ column of $B$. Consider the following
\[
  \begin{pmatrix} 1&2&3 \end{pmatrix} \begin{pmatrix} 4\\5\\6 \end{pmatrix} =(1\times 4)+(2\times 5)+(3\times 6)= 32
.\] 
In order to do matrix multiplication, rows of $A$ must match the columns of $B$ and vice versa. Matrix multiplication is not commutative. 
\begin{eg}
  If we let $A=\begin{pmatrix} 1&4\\5&10\\8&12 \end{pmatrix}$ and $B=\begin{pmatrix} -4&7&-3\\1&-3&2 \end{pmatrix} $. Does $AB=BA$? This is not true, because matrices are equal if and only if all entries are equal. If we just multiply $BA$, we get \[
  \begin{pmatrix} -4&7&-3\\1&-3&2 \end{pmatrix}\begin{pmatrix} 1&4\\5&10\\7&12 \end{pmatrix}  = \begin{pmatrix}2&8\\2&-2\end{pmatrix} 
  .\] 
\end{eg}
Here are some properties of matrices. 
\begin{note}
  We must maintain the order from left to right.
\end{note}
\begin{itemize}
  \item $A(B+C)=AB+BC$
  \item $(A+B)C=AC+BC$
  \item $k(AB)=(kA)B=A(kB)$
  \item $ABC=(AB)C=A(BC)$
\end{itemize}
The identity matrix is a square diagonal matrix with ones on the diagonal. For example, the identity matrix of dimension 3 and 4 are the following 
\begin{align*}
  I_3&=\begin{pmatrix} 1&0&0\\0&1&0\\0&0&1 \end{pmatrix} \\
  I_4&=\begin{pmatrix} 1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&0&1 \end{pmatrix} 
.\end{align*}
With the identity matrix, if we multiply any matrix by the identity matrix, then we get the same answer. Back in lesson 2, we were given the general solution to a Cauchy-Euler ordinary differential equation as \[
  x(t)=C_1(1)+C_2t^3+C_3 \frac{1}{t^2}
.\] Let's find a particular solution satisfying $x(1)=2,x'(1)=4,x'(1)=0$. We can do this by taking the derivative and second derivative of our functions to get 
\begin{align*}
  x'(t)=3C_2t^2-2C_3t ^{-3}\\
  x''(t)=6C_2t+6C_3t ^{-4}
.\end{align*}
Now using our initial conditions
\begin{align*}
  x(1)=C_1+C_2+C_3=2\\
  x'(1)=3C_2-2C_3=4\\
  x''(1)=6C_2+6C_3=0
.\end{align*}
We can rewrite this in matrix form as \[
  \begin{pmatrix} 1&1&1\\0&3&-2\\0&6&6 \end{pmatrix} \begin{pmatrix} C_1\\C_2\\C_3 \end{pmatrix} =\begin{pmatrix} 2\\4\\0 \end{pmatrix} 
.\] We will call the first matrix, $A$, the second matrix, $\vec{x}$, and the third matrix, $\vec{b}$, so we must solve $A\vec{x}=\vec{b}$ for $\vec{x}$. We can solve this by doing the following techniques
\begin{enumerate}
  \item Cramer's Rule (See book appendix)
  \item Gaussian elimination
  \item (Left)-multiply by $A^{-1}$
  \item MATLAB: rref or $A / B$
\end{enumerate}

\begin{theorem}
  This is the invertible matrix theorem. If $A$ is square, either all of case 1 or all of case 2 statements are true. Either $A$ is non-singular [good] or $A$ is singular [bad]. \newline Considering case 1
  \begin{itemize}
    \item $det(A)\neq 0$
    \item $A\vec{x}=\vec{b}$ has exactly one solution $\vec{x}$ for each $\vec{b}$.
    \item $A\vec{x}=\vec{0}$ has only the trivial solution, $\vec{x}=\vec{0}$.
    \item The rows (and columns) are linearly independent.
    \item $A^{-1}$ exists.
  \end{itemize}
  Considering Case 2
  \begin{itemize}
    \item $det(A)=0$
    \item $A\vec{x}=\vec{b}$ has zero or infinite solutions
    \item $A\vec{x}=\vec{0}$ has infinite solutions.
    \item The rows (and columns) are linearly dependent.
    \item $A^{-1}$ does not exist.
  \end{itemize}
\end{theorem}
$A^{-1}$ is a square matrix such that \[
A^{-1}A=A A^{-1}=I
.\] We find $A^{-1}$ through the following methods
\begin{enumerate}
  \item $rref(A|I)\to(I|A^{-1})$
  \item $A^{-1}=\frac{1}{det(A)}\cdot adj(A)$ where $adj(A)$ is the transpose of the cofactor matrix.
\end{enumerate}
How do we use $A^{-1}$? Given $A\vec{x}=\vec{b}$, we must solve for $x$. In order to do this, we left multiply by $A^{-1}$ to get 
\begin{align*}
  A^{-1}A\vec{x}&=A^{-1}\vec{b}\\
  I\vec{x}&=A^{-1}\vec{b}\\
  \vec{x}=A^{-1}\vec{b}
.\end{align*}
If given a general $2\times 2$ matrix, $A=\begin{pmatrix} a&b\\c&d \end{pmatrix} $, we can find the inverse by doing 
\begin{align*}
  A^{-1}&=\frac{1}{ad-bc}\begin{pmatrix} d&-c\\-b&a \end{pmatrix} ^{T}\\
        &=\frac{1}{ad-bc}\begin{pmatrix} d&-b\\-c&a \end{pmatrix} 
.\end{align*}
The trace of $A$ ($trace(A)$) is the sum along the diagonal.
\newline
Let's write a linear system of first order differential equations in matrix form. If the equation is not first order, use Sect (4.1) technique to turn into 1st order system.
\begin{eg}
  Consider the following system 
  \begin{align*}
    x'&=x-y+z\\
    y'&=4x+y-5z+7\\
    z'&=7x-8y-9z
  .\end{align*}
  The $+7$ in the second equation makes this a nonhomogeneous ordinary differential equation with constant coefficients. We can rewrite this as $\vec{x}'(t)=A(t)\vec{x}+\vec{b}(t)$, where $\vec{x}(t)=$ a vector of dependent variables. In order to do this, let $\vec{x}(t)=\begin{bmatrix} x(t)\\y(t)\\z(t) \end{bmatrix} $. Using the equation from before, we can set \[
  \begin{bmatrix} x'\\y'\\z'\\ \end{bmatrix} =\begin{bmatrix} 1&-1&1\\4&1&-5\\7&-8&-9 \end{bmatrix} +\begin{bmatrix} 0\\7\\0 \end{bmatrix} 
.\] We can notice that all of our nonhomogeneous terms get moved into the $\vec{b}(t)$ matrix.
\begin{note}
  We might have $\begin{bmatrix} c \end{bmatrix} \vec{x}'=A\vec{x}+\vec{b}$. From here we would need to do the following 
  \begin{align*}
    c^{-1}c\vec{x}'=c^{-1}(A\vec{x}+\vec{b})\\
    \vec{x}'=c^{-1}A\vec{x}+c^{-1}\vec{b}
  .\end{align*}
\end{note}
\end{eg}
\begin{eg}
  Consider the following ordinary differential equation and put it into matrix form.
  \begin{align*}
    x''+4x'-4x+4y&=0\\
    y''-4y'+5y-2x'+3x&=\sin(t)
  .\end{align*}
  In order to solve this we need to let $u=x'$, which makes $u'=x''$, and we need to let $v=y'$, which lets $u'=-4x'+4x-4y$ and $v'=y''=4y'-5y+2u-3x+\sin(t)$. This makes our system 
  \begin{align*}
    u'&=-4u+4x-4y\\
    v'&=4v-5y+2u-3x+\sin(t)\\
    x'&=u\\
    y'&=v
  .\end{align*}
  Now we can turn this into our $\vec{x'}=A\vec{x}+\vec{b}$. This makes our equation \[
    \begin{bmatrix} u'\\v'\\x'\\y'\\ \end{bmatrix} =\begin{bmatrix} -4&0&4&-4\\2&4&-3&-5\\1&0&0&0\\0&1&0&0 \end{bmatrix} +\begin{bmatrix} 0\\ \sin(t)\\0\\0 \end{bmatrix} 
  .\] 
\end{eg}

\subsection{Eigenvalues and Eigenvectors of Square Matrices}
We are going to be using $\lambda$ for the eigenvalues and $\vec{v}$ for the eigenvectors.
\begin{definition}
  $\lambda$ and $\vec{v}$ are an eigenvalue/eigenvector of $A$ if \[
  A\vec{v}=\lambda\vec{v}, v\neq 0
  .\] 
\end{definition}

Our steps to find the eigenvector are the following:
\begin{enumerate}
  \item Find $\lambda$'s with $det(A-\lambda I)=0$ (polynomial in $\lambda$).
  \item For each $\lambda$ find its eigenvector $\vec{v}$ using $A\vec{v}=\lambda\vec{v}$ or $(A-\lambda I)\vec{v}=0$
\end{enumerate}
We need to make sure that $\vec{v}$ is a non-zero solution to the equation. This means that we will have infinite solutions because every multiple of an eigenvector is also an eigenvector.
\begin{eg}
  For $A=\begin{bmatrix} 4&2\\5&1 \end{bmatrix}$, find eigenvalues and eigenvectors. In order to find the eigen vector we take the deteminant of $A-\lambda I$ and set it equal to zero.
  \begin{align*}
    det(\begin{bmatrix} 4&2\\5&1 \end{bmatrix} -\lambda\begin{bmatrix} 1&0\\0&1 \end{bmatrix} )&=0\\
    \left| \begin{matrix} 4-\lambda&2\\5&1-\lambda \end{matrix} \right| &=0\\
    (4-\lambda)(1-\lambda)-10&=0\\
    4-5\lambda+\lambda^2-10&=0\\
    \lambda^2-5\lambda-6&=0\\
    \lambda&=6,-1
  .\end{align*}
  Now we need to solve for the eigenvectors. First, let $\lambda=6$. Let $\vec{v}=\begin{bmatrix} v_1\\v_2 \end{bmatrix}$.
  \begin{align*}
    \begin{bmatrix} 4-6&2\\5&1-6 \end{bmatrix} \begin{bmatrix} v_1\\v_2 \end{bmatrix} &=\begin{bmatrix} 0\\0 \end{bmatrix} \\
    \begin{bmatrix} -2&2\\5&-5 \end{bmatrix} \begin{bmatrix} v_1\\v_2 \end{bmatrix} =\begin{bmatrix} 0\\0 \end{bmatrix} 
  .\end{align*}
  If we put this back into equation form we get 
  \begin{align*}
    -2v_1+2v_2&=0\\
    5v_1-5v_2&=0\\
    \to v_1=v_2
  .\end{align*}
  So this means that $v_2\begin{bmatrix} 1\\1 \end{bmatrix} $ is an eigenvalue for $\lambda=6$ for all $v_2\neq 0$. Let's now solve for $\lambda=1:$
  \begin{align*}
    \begin{bmatrix} 5&2\\5&2 \end{bmatrix} \begin{bmatrix} v_1\\v_2 \end{bmatrix} &=\begin{bmatrix} 0\\0 \end{bmatrix} \\
    5v_1-1v_2=0\to v_2=\frac{-5}{2}v_1
  .\end{align*}
  This means that any multiple of the vector $\begin{bmatrix} 1\\-\frac{5}{2} \end{bmatrix}$ is an eigenvector for $\lambda=1$
\end{eg}
